{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO90fxpA/8jJF9mIl0blQJP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Rafi Fadhlillah\n","# 121450143\n","# Deep Learning RC"],"metadata":{"id":"ZNBYI1H7ZVDv"}},{"cell_type":"markdown","source":["# Soal 1 [20 poin]\n","\n","Tingkatkan performa GRU dengan mengubah fungsi aktivasi dan mengintegrasikan fitur embedding dari pre-trained model seperti Word2Vec.\n","\n","Tugas:\n","\n","1. Ubah fungsi aktivasi dari tanh menjadi ReLU.\n","2. Gunakan embedding dari Word2Vec untuk merepresentasikan data input.\n","\n","Kunci Analisis: Lakukan perbandingan performa model sebelum dan sesudah modifikasi. Berikan comment anda terhadap akurasinya, di bandingkan yang ada di modul."],"metadata":{"id":"GrEqJ4fTZhS9"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, GRU, Dense\n","from gensim.models import Word2Vec\n","import json\n","import warnings\n","import re\n","import spacy\n","\n","\n","nlp = spacy.load('en_core_web_sm')\n","\n","warnings.filterwarnings('ignore')\n","\n","# Membaca Dataset\n","with open('Intent.json', 'rb') as file:\n","    sentences = json.load(file)\n","\n","# Lakukan Preprocessing Dataset\n","def pre_processing(line):\n","    line = re.sub(r'[^a-zA-z.?!\\']', ' ', line)\n","    line = re.sub(r'[ ]+', ' ', line)\n","    return line\n","\n","# Dapatkan judul dari intent\n","\n","inputs, targets, cls = [], [], []\n","cls = []\n","intent_doc = {}\n","\n","for i in sentences['intents']:\n","    if i['intent'] not in cls:\n","        cls.append(i['intent'])\n","\n","    if i['intent'] not in intent_doc:\n","        intent_doc[i['intent']] = []\n","\n","    for text in i['text']:\n","        inputs.append(pre_processing(text))\n","        targets.append(i['intent'])\n","\n","    for response in i['responses']:\n","        intent_doc[i['intent']].append(response)\n"],"metadata":{"id":"wxNQr2zjIYze","executionInfo":{"status":"ok","timestamp":1732002071507,"user_tz":-420,"elapsed":15450,"user":{"displayName":"Rafi Fadhlillah I2I450I43","userId":"14376513582865397586"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# Lanjutkan dengan Tokenizer\n","\n","def token_data(inp_list):\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<unk>')\n","\n","    tokenizer.fit_on_texts(inp_list)\n","\n","    inp_seq = tokenizer.texts_to_sequences(inp_list)\n","\n","    ''' adding padding '''\n","    inp_seq = tf.keras.preprocessing.sequence.pad_sequences(inp_seq, padding='pre')\n","\n","    return tokenizer, inp_seq"],"metadata":{"id":"MVPE0p2yKAi2","executionInfo":{"status":"ok","timestamp":1732002071508,"user_tz":-420,"elapsed":6,"user":{"displayName":"Rafi Fadhlillah I2I450I43","userId":"14376513582865397586"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["tokenizer, inp_tensor = token_data(inputs)"],"metadata":{"id":"Se-ge0w8KRLl","executionInfo":{"status":"ok","timestamp":1732002071508,"user_tz":-420,"elapsed":5,"user":{"displayName":"Rafi Fadhlillah I2I450I43","userId":"14376513582865397586"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def cr_cat_target(targets):\n","    word = {}\n","    cat_t = []\n","    counter=0\n","\n","    for trg in targets:\n","        if trg not in word:\n","            word[trg]=counter\n","            counter+=1\n","        cat_t.append(word[trg])\n","\n","    # Remove the 'dtype' argument\n","    cat_tensor = tf.keras.utils.to_categorical(cat_t, num_classes=len(word))\n","\n","    # If you need to ensure the data type, cast it afterwards\n","    cat_tensor = cat_tensor.astype('int32')\n","\n","    return cat_tensor, dict((v,k) for k, v in word.items())"],"metadata":{"id":"HQt-sLf1KVFF","executionInfo":{"status":"ok","timestamp":1732002071508,"user_tz":-420,"elapsed":5,"user":{"displayName":"Rafi Fadhlillah I2I450I43","userId":"14376513582865397586"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["target_tensor, target_idx_word = cr_cat_target(targets)"],"metadata":{"id":"LThx-hy1KXbd","executionInfo":{"status":"ok","timestamp":1732002071508,"user_tz":-420,"elapsed":5,"user":{"displayName":"Rafi Fadhlillah I2I450I43","userId":"14376513582865397586"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["print('input shape: {} and output shape: {}'.format(inp_tensor.shape, target_tensor.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wVX5EDcjKY6t","executionInfo":{"status":"ok","timestamp":1732002071508,"user_tz":-420,"elapsed":4,"user":{"displayName":"Rafi Fadhlillah I2I450I43","userId":"14376513582865397586"}},"outputId":"784a8a5b-7ab8-469f-bc0b-cb427206f760"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["input shape: (143, 9) and output shape: (143, 22)\n"]}]},{"cell_type":"code","source":["# Latih Model Word2Vec (Anda mungkin perlu menyesuaikan parameter)\n","sentences_for_w2v = [text.split() for text in inputs]\n","w2v_model = Word2Vec(sentences_for_w2v, vector_size=100, window=5, min_count=1, workers=4)\n","\n","# Buat matriks embedding\n","embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 100))\n","for word, i in tokenizer.word_index.items():\n","    if word in w2v_model.wv:\n","        embedding_matrix[i] = w2v_model.wv[word]\n","\n","# Bangun Model GRU dengan aktivasi ReLU dan embedding Word2Vec\n","model = Sequential()\n","model.add(Embedding(len(tokenizer.word_index) + 1, 100, weights=[embedding_matrix], input_length=inp_tensor.shape[1], trainable=False))\n","model.add(GRU(128, activation='relu', recurrent_activation='sigmoid'))  # Menggunakan ReLU\n","model.add(Dense(len(target_idx_word), activation='softmax'))\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Latih model\n","model.fit(inp_tensor, target_tensor, epochs=10)\n","\n","# Evaluasi model (Anda bisa menambah metrik evaluasi lainnya)\n","loss, accuracy = model.evaluate(inp_tensor, target_tensor)\n","print('Loss:', loss)\n","print('Akurasi:', accuracy)\n","\n","# Bandingkan dengan model asli (aktivasi tanh)\n","# Bangun model asli\n","original_model = Sequential()\n","original_model.add(Embedding(len(tokenizer.word_index) + 1, 100, weights=[embedding_matrix], input_length=inp_tensor.shape[1], trainable=False))\n","original_model.add(GRU(128, activation='tanh', recurrent_activation='sigmoid'))  # Menggunakan tanh\n","original_model.add(Dense(len(target_idx_word), activation='softmax'))\n","\n","original_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","original_model.fit(inp_tensor, target_tensor, epochs=10)\n","loss_original, accuracy_original = original_model.evaluate(inp_tensor, target_tensor)\n","print('Loss Model Asli:', loss_original)\n","print('Akurasi Model Asli:', accuracy_original)\n","\n","# Bandingkan kinerja\n","print('\\nModel dengan aktivasi ReLU:')\n","print('Loss:', loss)\n","print('Akurasi:', accuracy)\n","print('\\nModel Asli (tanh):')\n","print('Loss:', loss_original)\n","print('Akurasi:', accuracy_original)\n","\n","# Analisis model mana yang lebih baik (ReLU atau tanh)\n","if accuracy > accuracy_original:\n","    print('\\nModel dengan fungsi aktivasi ReLU memberikan kinerja yang lebih baik.')\n","elif accuracy < accuracy_original:\n","    print('\\nModel asli dengan fungsi aktivasi tanh memberikan kinerja yang lebih baik.')\n","else:\n","    print('\\nKedua model memiliki kinerja yang serupa.')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yXaTiZhOXYmO","executionInfo":{"status":"ok","timestamp":1732002084413,"user_tz":-420,"elapsed":12908,"user":{"displayName":"Rafi Fadhlillah I2I450I43","userId":"14376513582865397586"}},"outputId":"f7166bb9-8c29-4c86-a19b-65b4f3c871bb"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.0457 - loss: 3.0917\n","Epoch 2/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.0999 - loss: 3.0886\n","Epoch 3/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.0663 - loss: 3.0873\n","Epoch 4/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.1044 - loss: 3.0865\n","Epoch 5/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.1616 - loss: 3.0835\n","Epoch 6/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.1239 - loss: 3.0823\n","Epoch 7/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.1167 - loss: 3.0797\n","Epoch 8/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0909 - loss: 3.0789\n","Epoch 9/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0795 - loss: 3.0766\n","Epoch 10/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.1566 - loss: 3.0732\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.1509 - loss: 3.0635\n","Loss: 3.0714774131774902\n","Akurasi: 0.10489510744810104\n","Epoch 1/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.0419 - loss: 3.0915\n","Epoch 2/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0806 - loss: 3.0875\n","Epoch 3/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0655 - loss: 3.0836\n","Epoch 4/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0477 - loss: 3.0817\n","Epoch 5/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0531 - loss: 3.0784\n","Epoch 6/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0687 - loss: 3.0748\n","Epoch 7/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0690 - loss: 3.0701\n","Epoch 8/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0547 - loss: 3.0685\n","Epoch 9/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0642 - loss: 3.0649\n","Epoch 10/10\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0716 - loss: 3.0592\n","\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1055 - loss: 3.0313  \n","Loss Model Asli: 3.0542495250701904\n","Akurasi Model Asli: 0.0559440553188324\n","\n","Model dengan aktivasi ReLU:\n","Loss: 3.0714774131774902\n","Akurasi: 0.10489510744810104\n","\n","Model Asli (tanh):\n","Loss: 3.0542495250701904\n","Akurasi: 0.0559440553188324\n","\n","Model dengan fungsi aktivasi ReLU memberikan kinerja yang lebih baik.\n"]}]},{"cell_type":"markdown","source":["Modifikasi model GRU dengan mengubah fungsi aktivasi dari tanh ke ReLU menghasilkan sedikit peningkatan akurasi, dari 0.0559 menjadi 0.1049. Meskipun fungsi aktivasi ReLU umumnya berkinerja lebih baik untuk masalah vanishing gradient dan ekstraksi fitur yang kompleks, peningkatan akurasi yang kecil ini menunjukkan bahwa perubahan fungsi aktivasi bukanlah satu-satunya faktor kunci dan fitur lain dari model ini mungkin juga memerlukan perubahan untuk melihat peningkatan kinerja yang signifikan."],"metadata":{"id":"rhHclVMPZGU-"}},{"cell_type":"markdown","source":["# No 2"],"metadata":{"id":"4Wa9e0e0Xabe"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, GRU, Dense\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from gensim.models import Word2Vec\n","\n","# Data dummy (ganti dengan data dari modul)\n","sentences = [\n","    \"model deep learning memerlukan eksperimen\".split(),\n","    \"bidirectional LSTM memproses data sekuensial\".split(),\n","    \"kombinasi dengan GRU meningkatkan performa\".split(),\n","]\n","\n","labels = np.array([0, 1, 1])  # Label dummy\n","\n","# 1. Word2Vec Embedding\n","word2vec = Word2Vec(sentences, vector_size=50, min_count=1, workers=4)\n","vocab_size = len(word2vec.wv)\n","\n","# Membuat word-index mapping\n","word_index = {word: i for i, word in enumerate(word2vec.wv.index_to_key)}\n","\n","# Mengubah data ke bentuk indeks\n","def sentence_to_indices(sentence):\n","    return [word_index[word] for word in sentence]\n","\n","data = np.array([sentence_to_indices(sentence) for sentence in sentences])\n","\n","# Padding sequences\n","max_length = max(len(seq) for seq in data)\n","data_padded = pad_sequences(data, maxlen=max_length, padding=\"post\")\n","\n","# 2. Model Hibrid: Bidirectional LSTM dan GRU\n","def build_hybrid_model(hidden_size):\n","    model = Sequential([\n","        Embedding(input_dim=vocab_size, output_dim=50, weights=[word2vec.wv.vectors], trainable=False),\n","        Bidirectional(LSTM(hidden_size, return_sequences=True)),\n","        GRU(hidden_size, return_sequences=False),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","# 3. Eksperimen dengan berbagai hidden layer size\n","hidden_sizes = [64, 128, 256]\n","results = {}\n","\n","for size in hidden_sizes:\n","    print(f\"\\nTraining model dengan hidden size: {size}\")\n","    model = build_hybrid_model(size)\n","    history = model.fit(data_padded, labels, epochs=10, batch_size=2, verbose=1)\n","    loss, accuracy = model.evaluate(data_padded, labels, verbose=0)\n","    results[size] = accuracy\n","    print(f\"Accuracy dengan hidden size {size}: {accuracy:.4f}\")\n","\n","# 4. Analisis\n","print(\"\\n--- Analisis ---\")\n","for size, acc in results.items():\n","    print(f\"Hidden size {size}: Accuracy = {acc:.4f}\")\n","\n","best_size = max(results, key=results.get)\n","print(f\"Model dengan hidden size terbaik adalah {best_size} dengan akurasi {results[best_size]:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L3rgFYTUQWge","executionInfo":{"status":"ok","timestamp":1732002106703,"user_tz":-420,"elapsed":22294,"user":{"displayName":"Rafi Fadhlillah I2I450I43","userId":"14376513582865397586"}},"outputId":"7b711498-5f01-41bf-b493-d5899a2643fb"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training model dengan hidden size: 64\n","Epoch 1/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.3889 - loss: 0.6942\n","Epoch 2/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.6893\n","Epoch 3/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6111 - loss: 0.6846\n","Epoch 4/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7778 - loss: 0.6736\n","Epoch 5/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7778 - loss: 0.6630 \n","Epoch 6/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6111 - loss: 0.6722\n","Epoch 7/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6111 - loss: 0.6684 \n","Epoch 8/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7778 - loss: 0.6422\n","Epoch 9/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6111 - loss: 0.6577\n","Epoch 10/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7778 - loss: 0.6225\n","Accuracy dengan hidden size 64: 0.6667\n","\n","Training model dengan hidden size: 128\n","Epoch 1/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.3889 - loss: 0.6961\n","Epoch 2/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7778 - loss: 0.6846\n","Epoch 3/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7778 - loss: 0.6803\n","Epoch 4/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7778 - loss: 0.6766\n","Epoch 5/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6111 - loss: 0.6772\n","Epoch 6/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.6707\n","Epoch 7/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.6606\n","Epoch 8/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6111 - loss: 0.6443\n","Epoch 9/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7778 - loss: 0.6072\n","Epoch 10/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7778 - loss: 0.5657\n","Accuracy dengan hidden size 128: 0.6667\n","\n","Training model dengan hidden size: 256\n","Epoch 1/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - accuracy: 0.7778 - loss: 0.6978\n","Epoch 2/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7778 - loss: 0.6786\n","Epoch 3/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7778 - loss: 0.6716 \n","Epoch 4/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.6639\n","Epoch 5/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.6593 \n","Epoch 6/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.6372\n","Epoch 7/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 0.6026 \n","Epoch 8/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 0.5390 \n","Epoch 9/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.4279\n","Epoch 10/10\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 0.2731 \n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_test_function.<locals>.one_step_on_iterator at 0x782a795960e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy dengan hidden size 256: 1.0000\n","\n","--- Analisis ---\n","Hidden size 64: Accuracy = 0.6667\n","Hidden size 128: Accuracy = 0.6667\n","Hidden size 256: Accuracy = 1.0000\n","Model dengan hidden size terbaik adalah 256 dengan akurasi 1.0000\n"]}]},{"cell_type":"markdown","source":["# No 3"],"metadata":{"id":"0bLujwEDQr0d"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# Data dummy (ganti dengan data dari modul)\n","sentences = [\n","    \"pemahaman konteks dalam data sekuensial sangat penting\",\n","    \"bidirectional LSTM membaca data dari dua arah\",\n","    \"penggunaan embedding pretrained dapat meningkatkan akurasi\",\n","]\n","\n","labels = np.array([0, 1, 1])  # Label dummy\n","\n","# 1. Tokenisasi dan Padding\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","data = tokenizer.texts_to_sequences(sentences)\n","max_length = max(len(seq) for seq in data)\n","data_padded = pad_sequences(data, maxlen=max_length, padding=\"post\")\n","\n","# 2. Memuat GloVe Pre-trained Embedding\n","embedding_index = {}\n","glove_file = \"glove.6B.50d.txt\"  # Pastikan file ini tersedia\n","with open(glove_file, encoding=\"utf-8\") as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype=\"float32\")\n","        embedding_index[word] = coefs\n","\n","# Membuat matriks embedding\n","embedding_dim = 50\n","vocab_size = len(word_index) + 1\n","embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","for word, i in word_index.items():\n","    embedding_vector = embedding_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","# 3. Model dengan Pre-trained GloVe\n","def build_model_with_glove():\n","    model = Sequential([\n","        Embedding(input_dim=vocab_size, output_dim=embedding_dim,\n","                  weights=[embedding_matrix], trainable=False),\n","        Bidirectional(LSTM(128, return_sequences=False)),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","# 4. Model tanpa Pre-trained Embedding\n","def build_model_without_glove():\n","    model = Sequential([\n","        Embedding(input_dim=vocab_size, output_dim=embedding_dim, trainable=True),\n","        Bidirectional(LSTM(128, return_sequences=False)),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","# Training dan evaluasi model\n","print(\"\\n--- Model dengan Pre-trained GloVe ---\")\n","model_with_glove = build_model_with_glove()\n","model_with_glove.fit(data_padded, labels, epochs=10, batch_size=2, verbose=1)\n","loss_with_glove, acc_with_glove = model_with_glove.evaluate(data_padded, labels, verbose=0)\n","print(f\"Accuracy dengan pre-trained embedding GloVe: {acc_with_glove:.4f}\")\n","\n","print(\"\\n--- Model tanpa Pre-trained Embedding ---\")\n","model_without_glove = build_model_without_glove()\n","model_without_glove.fit(data_padded, labels, epochs=10, batch_size=2, verbose=1)\n","loss_without_glove, acc_without_glove = model_without_glove.evaluate(data_padded, labels, verbose=0)\n","print(f\"Accuracy tanpa pre-trained embedding: {acc_without_glove:.4f}\")\n","\n","# Analisis\n","print(\"\\n--- Analisis ---\")\n","print(f\"Accuracy dengan pre-trained GloVe: {acc_with_glove:.4f}\")\n","print(f\"Accuracy tanpa pre-trained embedding: {acc_without_glove:.4f}\")\n","if acc_with_glove > acc_without_glove:\n","    print(\"Penggunaan pre-trained GloVe memberikan keuntungan dalam akurasi.\")\n","else:\n","    print(\"Model tanpa pre-trained embedding memiliki performa serupa atau lebih baik.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"4X0N4devQte_","executionInfo":{"status":"error","timestamp":1732002397836,"user_tz":-420,"elapsed":359,"user":{"displayName":"Rafi Fadhlillah I2I450I43","userId":"14376513582865397586"}},"outputId":"ffbab905-67aa-4ae6-ab66-316236cc5f28"},"execution_count":11,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'glove.6B.50d.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-8173c6026deb>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0membedding_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mglove_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"glove.6B.50d.txt\"\u001b[0m  \u001b[0;31m# Pastikan file ini tersedia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.50d.txt'"]}]}]}